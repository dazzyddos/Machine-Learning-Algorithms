{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generic Gradient Descent from Scratch. i.e., performing gradient descent algorithms on\n",
    "#### features set with multiple variables to find out the optimum values of m vector of size n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We will be using boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### First step: Importing the necessary libraries\n",
    "import numpy as np   # for vector and matrix operations\n",
    "import pandas as pd  # for data analysis\n",
    "from sklearn import datasets  # we will be loading boston housing dataset from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()   # loading dataset into boston variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data: contains the actual data that we will be feeding into our model for training and predicting\n",
    "#### target: contains the result values or say answers\n",
    "#### feature_name: is the list of all feature name :p .\n",
    "#### DESCR: contains the description of the data and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's load/convert our data into DataFrame object to perform some analysis\n",
    "### np.c_ for concatenation\n",
    "### np.append for appending the 'target' string to the feature_name list\n",
    "boston_df = pd.DataFrame(np.c_[boston['data'], boston['target']], columns=np.append(boston['feature_names'], 'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head()    # looking at the first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.shape   # dimension of our datasets i.e., number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT      target  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.describe()    # let's have some description of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "target     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.isnull().sum()   # let's see how many null entries are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### we have no NaNs in our dataset as all the datasets from sklearn library are already cleaned\n",
    "#### But in real life cases you will encounter many NaNs and you have to decide how you wanna\n",
    "#### deal with those. You can either delete that row or fill them with median, mean or mode ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Let's split our dataset into X and y\n",
    "#### X containing all the features\n",
    "#### y containing the target or results\n",
    "X = boston_df.iloc[:, :-1].values      # all the data except for the last column\n",
    "y = boston_df.iloc[:, -1].values       # only the last column\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13), (404,), (102,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Now that we have our X and y\n",
    "#### Let's split them into training and test set\n",
    "#### We'll train our model using the training set\n",
    "#### And we'll use test set for testing the performance of our model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38676786 -0.49559343 -0.60928978 ..., -0.24857777  0.28674182\n",
      "  -0.96685016]\n",
      " [-0.38534946  0.57923879 -0.86952633 ...,  0.58214721  0.36669519\n",
      "  -0.82116789]\n",
      " [ 1.43910768 -0.49559343  1.02669166 ...,  0.81290414  0.43472666\n",
      "   2.50177533]\n",
      " ..., \n",
      " [ 0.24089158 -0.49559343  1.02669166 ...,  0.81290414  0.43472666\n",
      "   0.9145323 ]\n",
      " [-0.36607242 -0.49559343 -0.713092   ..., -0.47933471  0.21433534\n",
      "  -0.26341291]\n",
      " [-0.39348854 -0.49559343 -0.74818007 ...,  0.35139027  0.43472666\n",
      "  -0.55616491]]\n"
     ]
    }
   ],
   "source": [
    "#### Let's apply feature scaling on our dataset\n",
    "#### So that it will be easy for calculating gradient\n",
    "#### And also all features should be in one range\n",
    "#### So that the model doesn't learn one feature with high values should be given higher priority\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As we can see the values are now between -1 to 1\n",
    "#### A beginner would be thinking why don't we apply feature scaling before splitting the\n",
    "#### dataset into training and test set. Yes you should think if you are a deep learner like\n",
    "#### me ;) . See the model we make should be given the exact type data that we had used while\n",
    "#### training our model. But when we deploy our model on web or on software we don't know what \n",
    "#### the user might be providing. When we are performing scaling on our training dataset\n",
    "#### it learns some parameters i.e., std and mean. And the same parameters should be used\n",
    "#### while transforming the testing data also. That's why we perform fit_transform on training\n",
    "#### data to tell it to learn the parameters and also transform and only transform on testing \n",
    "#### data to tell it to only transform it using the parameters learned through training set.\n",
    "#### Hope it cleared it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now let's start coding the main crux algorithm part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Here's the main stuff going on\n",
    "#### First it is iterating through the dataset row by row\n",
    "#### performing the matrix dot product on ith row and the m vector\n",
    "#### Then calculating the slope or gradient\n",
    "#### And finally we are subtracting the slope from the actual values of m\n",
    "#### if the slope is negative then it will get added else subtracted\n",
    "def step_gradient(data, Y, learning_rate, m):\n",
    "    N = data.shape[1]    # number of columns\n",
    "    m_slope = np.zeros(N)   # m_slope array contains all 0s initially\n",
    "    M = len(data)  # number of rows\n",
    "    for i in range(M):\n",
    "        x = data[i, :]  # ith training dataset\n",
    "        y = Y[i]  # ith target vale\n",
    "        mx = np.dot(x.T, m)  # dot product on ith training dataset and m vector\n",
    "        \n",
    "        for j in range(N):   # iterating through the columns of ith training set and finding the slope\n",
    "            m_slope[j] += (-2/M) * (y - mx) * x[j]\n",
    "            \n",
    "        new_m = m - learning_rate * m_slope\n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(data, Y, m):\n",
    "    total_cost = 0\n",
    "    M = len(data)\n",
    "    for i in range(M):\n",
    "        x = data[i]\n",
    "        y = Y[i]\n",
    "        total_cost += (1/M)*((y - (m*x).sum() )**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(data, Y, num_iterations, learning_rate):\n",
    "    N = data.shape[1]   # number of columns\n",
    "    m = np.zeros(N)     # creating an empty array of size M which will initially contain all 0s\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        m = step_gradient(data, Y, learning_rate, m)  # call the function to find the slope after going through the whole dataset\n",
    "        print(\"Cost at \", i, \" :\", cost(data, Y, m))  # at the same time also print the cost so that we can check if is it improving or not\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be the starting point of our algorithm\n",
    "# We will initialize the hyperparameters for gradient descent in this function\n",
    "# You need to tweak the values of the hyperparameters to adjust algorithm\n",
    "# When this function is done executing we'll have optimum values of m\n",
    "# i.e., values of m at which the cost is minimum\n",
    "def run(data, y):\n",
    "    num_iterations = 500    # epoch i.e., how many times to iterate through whole dataset\n",
    "    learning_rate = 0.1   # learning rate to avoid overshooting or slow learning\n",
    "    m = gd(data, y, num_iterations, learning_rate) # optimum m values :D\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Before calling the gradient descent \n",
    "### We must add an extra columns containing all 1s\n",
    "### We are doing this so that we don't have to find the value of c from the linear equation\n",
    "### y = mx + c. We just want the equation to be y = mx. An extra m will be treated as c as anything\n",
    "### multiplied to 1 is that number itself.\n",
    "ones_col = np.ones(len(X_train_scaled))\n",
    "ones_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We must reshape it into 2D array to append it into our dataset\n",
    "ones_col = ones_col.reshape(-1, 1)\n",
    "final_X = np.append(X_train_scaled, ones_col, axis=1)\n",
    "final_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at  0  : 365.256337774\n",
      "Cost at  1  : 241.045139325\n",
      "Cost at  2  : 162.747955173\n",
      "Cost at  3  : 112.794311736\n",
      "Cost at  4  : 80.854549171\n",
      "Cost at  5  : 60.4017800097\n",
      "Cost at  6  : 47.2820989203\n",
      "Cost at  7  : 38.8479982297\n",
      "Cost at  8  : 33.410585434\n",
      "Cost at  9  : 29.8916533493\n",
      "Cost at  10  : 27.6023661463\n",
      "Cost at  11  : 26.1023377005\n",
      "Cost at  12  : 25.1098200932\n",
      "Cost at  13  : 24.4444209428\n",
      "Cost at  14  : 23.9905428734\n",
      "Cost at  15  : 23.6740345195\n",
      "Cost at  16  : 23.4472706303\n",
      "Cost at  17  : 23.2796134112\n",
      "Cost at  18  : 23.1513112426\n",
      "Cost at  19  : 23.0495942064\n",
      "Cost at  20  : 22.9661742375\n",
      "Cost at  21  : 22.8956437732\n",
      "Cost at  22  : 22.8344493891\n",
      "Cost at  23  : 22.7802335441\n",
      "Cost at  24  : 22.7314120901\n",
      "Cost at  25  : 22.6869028515\n",
      "Cost at  26  : 22.6459510506\n",
      "Cost at  27  : 22.6080168538\n",
      "Cost at  28  : 22.5727027888\n",
      "Cost at  29  : 22.5397067729\n",
      "Cost at  30  : 22.5087916088\n",
      "Cost at  31  : 22.4797650816\n",
      "Cost at  32  : 22.4524668898\n",
      "Cost at  33  : 22.4267599938\n",
      "Cost at  34  : 22.402524824\n",
      "Cost at  35  : 22.3796553495\n",
      "Cost at  36  : 22.358056362\n",
      "Cost at  37  : 22.3376415574\n",
      "Cost at  38  : 22.3183321473\n",
      "Cost at  39  : 22.3000558243\n",
      "Cost at  40  : 22.2827459683\n",
      "Cost at  41  : 22.2663410184\n",
      "Cost at  42  : 22.2507839621\n",
      "Cost at  43  : 22.2360219085\n",
      "Cost at  44  : 22.2220057249\n",
      "Cost at  45  : 22.20868972\n",
      "Cost at  46  : 22.196031365\n",
      "Cost at  47  : 22.1839910456\n",
      "Cost at  48  : 22.1725318374\n",
      "Cost at  49  : 22.1616193038\n",
      "Cost at  50  : 22.1512213113\n",
      "Cost at  51  : 22.1413078611\n",
      "Cost at  52  : 22.1318509343\n",
      "Cost at  53  : 22.1228243497\n",
      "Cost at  54  : 22.1142036337\n",
      "Cost at  55  : 22.1059658988\n",
      "Cost at  56  : 22.0980897331\n",
      "Cost at  57  : 22.0905550968\n",
      "Cost at  58  : 22.0833432273\n",
      "Cost at  59  : 22.0764365513\n",
      "Cost at  60  : 22.0698186027\n",
      "Cost at  61  : 22.0634739475\n",
      "Cost at  62  : 22.0573881132\n",
      "Cost at  63  : 22.0515475239\n",
      "Cost at  64  : 22.0459394402\n",
      "Cost at  65  : 22.0405519023\n",
      "Cost at  66  : 22.0353736788\n",
      "Cost at  67  : 22.0303942176\n",
      "Cost at  68  : 22.025603601\n",
      "Cost at  69  : 22.0209925042\n",
      "Cost at  70  : 22.0165521558\n",
      "Cost at  71  : 22.0122743018\n",
      "Cost at  72  : 22.008151172\n",
      "Cost at  73  : 22.0041754482\n",
      "Cost at  74  : 22.0003402354\n",
      "Cost at  75  : 21.9966390342\n",
      "Cost at  76  : 21.9930657153\n",
      "Cost at  77  : 21.9896144964\n",
      "Cost at  78  : 21.9862799195\n",
      "Cost at  79  : 21.9830568308\n",
      "Cost at  80  : 21.9799403612\n",
      "Cost at  81  : 21.9769259086\n",
      "Cost at  82  : 21.9740091209\n",
      "Cost at  83  : 21.9711858809\n",
      "Cost at  84  : 21.9684522915\n",
      "Cost at  85  : 21.965804662\n",
      "Cost at  86  : 21.9632394955\n",
      "Cost at  87  : 21.9607534772\n",
      "Cost at  88  : 21.9583434633\n",
      "Cost at  89  : 21.9560064706\n",
      "Cost at  90  : 21.9537396667\n",
      "Cost at  91  : 21.9515403612\n",
      "Cost at  92  : 21.9494059973\n",
      "Cost at  93  : 21.9473341436\n",
      "Cost at  94  : 21.9453224871\n",
      "Cost at  95  : 21.9433688259\n",
      "Cost at  96  : 21.9414710632\n",
      "Cost at  97  : 21.9396272007\n",
      "Cost at  98  : 21.9378353334\n",
      "Cost at  99  : 21.9360936441\n",
      "Cost at  100  : 21.9344003984\n",
      "Cost at  101  : 21.9327539403\n",
      "Cost at  102  : 21.9311526874\n",
      "Cost at  103  : 21.9295951271\n",
      "Cost at  104  : 21.9280798129\n",
      "Cost at  105  : 21.9266053605\n",
      "Cost at  106  : 21.9251704446\n",
      "Cost at  107  : 21.9237737956\n",
      "Cost at  108  : 21.922414197\n",
      "Cost at  109  : 21.921090482\n",
      "Cost at  110  : 21.9198015315\n",
      "Cost at  111  : 21.9185462712\n",
      "Cost at  112  : 21.9173236696\n",
      "Cost at  113  : 21.9161327357\n",
      "Cost at  114  : 21.9149725167\n",
      "Cost at  115  : 21.9138420966\n",
      "Cost at  116  : 21.9127405941\n",
      "Cost at  117  : 21.9116671606\n",
      "Cost at  118  : 21.9106209795\n",
      "Cost at  119  : 21.9096012634\n",
      "Cost at  120  : 21.9086072541\n",
      "Cost at  121  : 21.90763822\n",
      "Cost at  122  : 21.9066934556\n",
      "Cost at  123  : 21.9057722801\n",
      "Cost at  124  : 21.9048740363\n",
      "Cost at  125  : 21.9039980894\n",
      "Cost at  126  : 21.9031438261\n",
      "Cost at  127  : 21.9023106539\n",
      "Cost at  128  : 21.9014979996\n",
      "Cost at  129  : 21.9007053091\n",
      "Cost at  130  : 21.8999320461\n",
      "Cost at  131  : 21.8991776917\n",
      "Cost at  132  : 21.8984417435\n",
      "Cost at  133  : 21.8977237149\n",
      "Cost at  134  : 21.8970231345\n",
      "Cost at  135  : 21.8963395457\n",
      "Cost at  136  : 21.8956725056\n",
      "Cost at  137  : 21.895021585\n",
      "Cost at  138  : 21.8943863675\n",
      "Cost at  139  : 21.8937664493\n",
      "Cost at  140  : 21.8931614385\n",
      "Cost at  141  : 21.8925709546\n",
      "Cost at  142  : 21.8919946284\n",
      "Cost at  143  : 21.8914321012\n",
      "Cost at  144  : 21.8908830248\n",
      "Cost at  145  : 21.8903470608\n",
      "Cost at  146  : 21.8898238803\n",
      "Cost at  147  : 21.8893131639\n",
      "Cost at  148  : 21.8888146009\n",
      "Cost at  149  : 21.8883278892\n",
      "Cost at  150  : 21.8878527353\n",
      "Cost at  151  : 21.8873888534\n",
      "Cost at  152  : 21.8869359658\n",
      "Cost at  153  : 21.8864938019\n",
      "Cost at  154  : 21.8860620989\n",
      "Cost at  155  : 21.8856406006\n",
      "Cost at  156  : 21.8852290578\n",
      "Cost at  157  : 21.8848272279\n",
      "Cost at  158  : 21.8844348747\n",
      "Cost at  159  : 21.8840517682\n",
      "Cost at  160  : 21.8836776844\n",
      "Cost at  161  : 21.8833124049\n",
      "Cost at  162  : 21.8829557174\n",
      "Cost at  163  : 21.8826074146\n",
      "Cost at  164  : 21.8822672949\n",
      "Cost at  165  : 21.8819351616\n",
      "Cost at  166  : 21.8816108232\n",
      "Cost at  167  : 21.8812940928\n",
      "Cost at  168  : 21.8809847884\n",
      "Cost at  169  : 21.8806827327\n",
      "Cost at  170  : 21.8803877525\n",
      "Cost at  171  : 21.8800996794\n",
      "Cost at  172  : 21.8798183487\n",
      "Cost at  173  : 21.8795436002\n",
      "Cost at  174  : 21.8792752775\n",
      "Cost at  175  : 21.8790132281\n",
      "Cost at  176  : 21.8787573032\n",
      "Cost at  177  : 21.8785073577\n",
      "Cost at  178  : 21.8782632502\n",
      "Cost at  179  : 21.8780248426\n",
      "Cost at  180  : 21.8777920002\n",
      "Cost at  181  : 21.8775645915\n",
      "Cost at  182  : 21.8773424885\n",
      "Cost at  183  : 21.877125566\n",
      "Cost at  184  : 21.876913702\n",
      "Cost at  185  : 21.8767067774\n",
      "Cost at  186  : 21.8765046759\n",
      "Cost at  187  : 21.8763072842\n",
      "Cost at  188  : 21.8761144916\n",
      "Cost at  189  : 21.8759261901\n",
      "Cost at  190  : 21.8757422742\n",
      "Cost at  191  : 21.875562641\n",
      "Cost at  192  : 21.8753871901\n",
      "Cost at  193  : 21.8752158235\n",
      "Cost at  194  : 21.8750484455\n",
      "Cost at  195  : 21.8748849626\n",
      "Cost at  196  : 21.8747252836\n",
      "Cost at  197  : 21.8745693196\n",
      "Cost at  198  : 21.8744169837\n",
      "Cost at  199  : 21.874268191\n",
      "Cost at  200  : 21.8741228587\n",
      "Cost at  201  : 21.8739809059\n",
      "Cost at  202  : 21.8738422537\n",
      "Cost at  203  : 21.873706825\n",
      "Cost at  204  : 21.8735745445\n",
      "Cost at  205  : 21.8734453388\n",
      "Cost at  206  : 21.8733191361\n",
      "Cost at  207  : 21.8731958664\n",
      "Cost at  208  : 21.8730754612\n",
      "Cost at  209  : 21.8729578538\n",
      "Cost at  210  : 21.8728429789\n",
      "Cost at  211  : 21.8727307729\n",
      "Cost at  212  : 21.8726211736\n",
      "Cost at  213  : 21.8725141202\n",
      "Cost at  214  : 21.8724095535\n",
      "Cost at  215  : 21.8723074155\n",
      "Cost at  216  : 21.8722076497\n",
      "Cost at  217  : 21.8721102008\n",
      "Cost at  218  : 21.8720150149\n",
      "Cost at  219  : 21.8719220394\n",
      "Cost at  220  : 21.8718312229\n",
      "Cost at  221  : 21.871742515\n",
      "Cost at  222  : 21.8716558667\n",
      "Cost at  223  : 21.8715712302\n",
      "Cost at  224  : 21.8714885586\n",
      "Cost at  225  : 21.8714078063\n",
      "Cost at  226  : 21.8713289285\n",
      "Cost at  227  : 21.8712518818\n",
      "Cost at  228  : 21.8711766236\n",
      "Cost at  229  : 21.8711031122\n",
      "Cost at  230  : 21.8710313072\n",
      "Cost at  231  : 21.8709611687\n",
      "Cost at  232  : 21.8708926582\n",
      "Cost at  233  : 21.8708257377\n",
      "Cost at  234  : 21.8707603703\n",
      "Cost at  235  : 21.87069652\n",
      "Cost at  236  : 21.8706341515\n",
      "Cost at  237  : 21.8705732304\n",
      "Cost at  238  : 21.870513723\n",
      "Cost at  239  : 21.8704555965\n",
      "Cost at  240  : 21.8703988188\n",
      "Cost at  241  : 21.8703433587\n",
      "Cost at  242  : 21.8702891855\n",
      "Cost at  243  : 21.8702362693\n",
      "Cost at  244  : 21.870184581\n",
      "Cost at  245  : 21.870134092\n",
      "Cost at  246  : 21.8700847745\n",
      "Cost at  247  : 21.8700366012\n",
      "Cost at  248  : 21.8699895457\n",
      "Cost at  249  : 21.869943582\n",
      "Cost at  250  : 21.8698986847\n",
      "Cost at  251  : 21.869854829\n",
      "Cost at  252  : 21.8698119909\n",
      "Cost at  253  : 21.8697701466\n",
      "Cost at  254  : 21.8697292731\n",
      "Cost at  255  : 21.8696893479\n",
      "Cost at  256  : 21.8696503489\n",
      "Cost at  257  : 21.8696122547\n",
      "Cost at  258  : 21.8695750443\n",
      "Cost at  259  : 21.8695386971\n",
      "Cost at  260  : 21.8695031932\n",
      "Cost at  261  : 21.8694685128\n",
      "Cost at  262  : 21.8694346371\n",
      "Cost at  263  : 21.8694015472\n",
      "Cost at  264  : 21.8693692249\n",
      "Cost at  265  : 21.8693376524\n",
      "Cost at  266  : 21.8693068124\n",
      "Cost at  267  : 21.8692766877\n",
      "Cost at  268  : 21.8692472619\n",
      "Cost at  269  : 21.8692185187\n",
      "Cost at  270  : 21.8691904423\n",
      "Cost at  271  : 21.8691630172\n",
      "Cost at  272  : 21.8691362282\n",
      "Cost at  273  : 21.8691100607\n",
      "Cost at  274  : 21.8690845002\n",
      "Cost at  275  : 21.8690595326\n",
      "Cost at  276  : 21.8690351442\n",
      "Cost at  277  : 21.8690113215\n",
      "Cost at  278  : 21.8689880514\n",
      "Cost at  279  : 21.8689653211\n",
      "Cost at  280  : 21.868943118\n",
      "Cost at  281  : 21.86892143\n",
      "Cost at  282  : 21.8689002451\n",
      "Cost at  283  : 21.8688795515\n",
      "Cost at  284  : 21.868859338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at  285  : 21.8688395934\n",
      "Cost at  286  : 21.8688203067\n",
      "Cost at  287  : 21.8688014674\n",
      "Cost at  288  : 21.8687830651\n",
      "Cost at  289  : 21.8687650897\n",
      "Cost at  290  : 21.8687475312\n",
      "Cost at  291  : 21.86873038\n",
      "Cost at  292  : 21.8687136267\n",
      "Cost at  293  : 21.8686972619\n",
      "Cost at  294  : 21.8686812767\n",
      "Cost at  295  : 21.8686656623\n",
      "Cost at  296  : 21.8686504101\n",
      "Cost at  297  : 21.8686355117\n",
      "Cost at  298  : 21.8686209589\n",
      "Cost at  299  : 21.8686067436\n",
      "Cost at  300  : 21.868592858\n",
      "Cost at  301  : 21.8685792945\n",
      "Cost at  302  : 21.8685660456\n",
      "Cost at  303  : 21.8685531041\n",
      "Cost at  304  : 21.8685404627\n",
      "Cost at  305  : 21.8685281145\n",
      "Cost at  306  : 21.8685160528\n",
      "Cost at  307  : 21.8685042708\n",
      "Cost at  308  : 21.8684927621\n",
      "Cost at  309  : 21.8684815204\n",
      "Cost at  310  : 21.8684705394\n",
      "Cost at  311  : 21.8684598131\n",
      "Cost at  312  : 21.8684493356\n",
      "Cost at  313  : 21.8684391011\n",
      "Cost at  314  : 21.868429104\n",
      "Cost at  315  : 21.8684193388\n",
      "Cost at  316  : 21.8684098001\n",
      "Cost at  317  : 21.8684004827\n",
      "Cost at  318  : 21.8683913814\n",
      "Cost at  319  : 21.8683824911\n",
      "Cost at  320  : 21.8683738071\n",
      "Cost at  321  : 21.8683653245\n",
      "Cost at  322  : 21.8683570387\n",
      "Cost at  323  : 21.868348945\n",
      "Cost at  324  : 21.8683410391\n",
      "Cost at  325  : 21.8683333166\n",
      "Cost at  326  : 21.8683257731\n",
      "Cost at  327  : 21.8683184047\n",
      "Cost at  328  : 21.8683112071\n",
      "Cost at  329  : 21.8683041765\n",
      "Cost at  330  : 21.868297309\n",
      "Cost at  331  : 21.8682906008\n",
      "Cost at  332  : 21.8682840481\n",
      "Cost at  333  : 21.8682776475\n",
      "Cost at  334  : 21.8682713953\n",
      "Cost at  335  : 21.8682652881\n",
      "Cost at  336  : 21.8682593226\n",
      "Cost at  337  : 21.8682534954\n",
      "Cost at  338  : 21.8682478034\n",
      "Cost at  339  : 21.8682422434\n",
      "Cost at  340  : 21.8682368124\n",
      "Cost at  341  : 21.8682315074\n",
      "Cost at  342  : 21.8682263254\n",
      "Cost at  343  : 21.8682212636\n",
      "Cost at  344  : 21.8682163192\n",
      "Cost at  345  : 21.8682114895\n",
      "Cost at  346  : 21.8682067718\n",
      "Cost at  347  : 21.8682021636\n",
      "Cost at  348  : 21.8681976622\n",
      "Cost at  349  : 21.8681932652\n",
      "Cost at  350  : 21.8681889702\n",
      "Cost at  351  : 21.8681847749\n",
      "Cost at  352  : 21.8681806768\n",
      "Cost at  353  : 21.8681766738\n",
      "Cost at  354  : 21.8681727637\n",
      "Cost at  355  : 21.8681689442\n",
      "Cost at  356  : 21.8681652134\n",
      "Cost at  357  : 21.868161569\n",
      "Cost at  358  : 21.8681580092\n",
      "Cost at  359  : 21.868154532\n",
      "Cost at  360  : 21.8681511354\n",
      "Cost at  361  : 21.8681478176\n",
      "Cost at  362  : 21.8681445768\n",
      "Cost at  363  : 21.8681414111\n",
      "Cost at  364  : 21.8681383189\n",
      "Cost at  365  : 21.8681352983\n",
      "Cost at  366  : 21.8681323479\n",
      "Cost at  367  : 21.8681294659\n",
      "Cost at  368  : 21.8681266507\n",
      "Cost at  369  : 21.8681239008\n",
      "Cost at  370  : 21.8681212147\n",
      "Cost at  371  : 21.8681185909\n",
      "Cost at  372  : 21.868116028\n",
      "Cost at  373  : 21.8681135245\n",
      "Cost at  374  : 21.868111079\n",
      "Cost at  375  : 21.8681086903\n",
      "Cost at  376  : 21.868106357\n",
      "Cost at  377  : 21.8681040778\n",
      "Cost at  378  : 21.8681018515\n",
      "Cost at  379  : 21.8680996769\n",
      "Cost at  380  : 21.8680975526\n",
      "Cost at  381  : 21.8680954776\n",
      "Cost at  382  : 21.8680934508\n",
      "Cost at  383  : 21.868091471\n",
      "Cost at  384  : 21.8680895371\n",
      "Cost at  385  : 21.868087648\n",
      "Cost at  386  : 21.8680858028\n",
      "Cost at  387  : 21.8680840004\n",
      "Cost at  388  : 21.8680822397\n",
      "Cost at  389  : 21.8680805199\n",
      "Cost at  390  : 21.86807884\n",
      "Cost at  391  : 21.8680771991\n",
      "Cost at  392  : 21.8680755962\n",
      "Cost at  393  : 21.8680740305\n",
      "Cost at  394  : 21.8680725011\n",
      "Cost at  395  : 21.8680710072\n",
      "Cost at  396  : 21.868069548\n",
      "Cost at  397  : 21.8680681226\n",
      "Cost at  398  : 21.8680667302\n",
      "Cost at  399  : 21.8680653701\n",
      "Cost at  400  : 21.8680640416\n",
      "Cost at  401  : 21.8680627439\n",
      "Cost at  402  : 21.8680614763\n",
      "Cost at  403  : 21.8680602382\n",
      "Cost at  404  : 21.8680590287\n",
      "Cost at  405  : 21.8680578473\n",
      "Cost at  406  : 21.8680566932\n",
      "Cost at  407  : 21.868055566\n",
      "Cost at  408  : 21.8680544649\n",
      "Cost at  409  : 21.8680533893\n",
      "Cost at  410  : 21.8680523387\n",
      "Cost at  411  : 21.8680513124\n",
      "Cost at  412  : 21.86805031\n",
      "Cost at  413  : 21.8680493308\n",
      "Cost at  414  : 21.8680483743\n",
      "Cost at  415  : 21.86804744\n",
      "Cost at  416  : 21.8680465274\n",
      "Cost at  417  : 21.8680456359\n",
      "Cost at  418  : 21.8680447651\n",
      "Cost at  419  : 21.8680439146\n",
      "Cost at  420  : 21.8680430837\n",
      "Cost at  421  : 21.8680422721\n",
      "Cost at  422  : 21.8680414793\n",
      "Cost at  423  : 21.868040705\n",
      "Cost at  424  : 21.8680399486\n",
      "Cost at  425  : 21.8680392097\n",
      "Cost at  426  : 21.868038488\n",
      "Cost at  427  : 21.868037783\n",
      "Cost at  428  : 21.8680370943\n",
      "Cost at  429  : 21.8680364217\n",
      "Cost at  430  : 21.8680357646\n",
      "Cost at  431  : 21.8680351228\n",
      "Cost at  432  : 21.8680344958\n",
      "Cost at  433  : 21.8680338834\n",
      "Cost at  434  : 21.8680332853\n",
      "Cost at  435  : 21.8680327009\n",
      "Cost at  436  : 21.8680321302\n",
      "Cost at  437  : 21.8680315727\n",
      "Cost at  438  : 21.8680310281\n",
      "Cost at  439  : 21.8680304961\n",
      "Cost at  440  : 21.8680299765\n",
      "Cost at  441  : 21.8680294689\n",
      "Cost at  442  : 21.8680289731\n",
      "Cost at  443  : 21.8680284888\n",
      "Cost at  444  : 21.8680280157\n",
      "Cost at  445  : 21.8680275537\n",
      "Cost at  446  : 21.8680271023\n",
      "Cost at  447  : 21.8680266614\n",
      "Cost at  448  : 21.8680262307\n",
      "Cost at  449  : 21.86802581\n",
      "Cost at  450  : 21.8680253991\n",
      "Cost at  451  : 21.8680249977\n",
      "Cost at  452  : 21.8680246056\n",
      "Cost at  453  : 21.8680242226\n",
      "Cost at  454  : 21.8680238485\n",
      "Cost at  455  : 21.868023483\n",
      "Cost at  456  : 21.8680231261\n",
      "Cost at  457  : 21.8680227774\n",
      "Cost at  458  : 21.8680224368\n",
      "Cost at  459  : 21.8680221041\n",
      "Cost at  460  : 21.8680217792\n",
      "Cost at  461  : 21.8680214617\n",
      "Cost at  462  : 21.8680211516\n",
      "Cost at  463  : 21.8680208488\n",
      "Cost at  464  : 21.8680205529\n",
      "Cost at  465  : 21.8680202639\n",
      "Cost at  466  : 21.8680199816\n",
      "Cost at  467  : 21.8680197059\n",
      "Cost at  468  : 21.8680194365\n",
      "Cost at  469  : 21.8680191734\n",
      "Cost at  470  : 21.8680189164\n",
      "Cost at  471  : 21.8680186654\n",
      "Cost at  472  : 21.8680184202\n",
      "Cost at  473  : 21.8680181806\n",
      "Cost at  474  : 21.8680179467\n",
      "Cost at  475  : 21.8680177181\n",
      "Cost at  476  : 21.8680174949\n",
      "Cost at  477  : 21.8680172768\n",
      "Cost at  478  : 21.8680170638\n",
      "Cost at  479  : 21.8680168557\n",
      "Cost at  480  : 21.8680166525\n",
      "Cost at  481  : 21.868016454\n",
      "Cost at  482  : 21.86801626\n",
      "Cost at  483  : 21.8680160706\n",
      "Cost at  484  : 21.8680158856\n",
      "Cost at  485  : 21.8680157049\n",
      "Cost at  486  : 21.8680155283\n",
      "Cost at  487  : 21.8680153559\n",
      "Cost at  488  : 21.8680151874\n",
      "Cost at  489  : 21.8680150229\n",
      "Cost at  490  : 21.8680148621\n",
      "Cost at  491  : 21.8680147051\n",
      "Cost at  492  : 21.8680145518\n",
      "Cost at  493  : 21.868014402\n",
      "Cost at  494  : 21.8680142556\n",
      "Cost at  495  : 21.8680141127\n",
      "Cost at  496  : 21.8680139731\n",
      "Cost at  497  : 21.8680138367\n",
      "Cost at  498  : 21.8680137035\n",
      "Cost at  499  : 21.8680135734\n"
     ]
    }
   ],
   "source": [
    "### I think we are almost ready\n",
    "### Let's run our algorithm\n",
    "m = run(final_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.01600053,   1.34870013,   0.12619247,   0.57643535,\n",
       "        -2.28989842,   2.12620331,   0.12938467,  -3.17807284,\n",
       "         2.63684855,  -1.86959845,  -2.14517014,   0.67879373,\n",
       "        -3.93214294,  22.52227723])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we have our optimum values of m that we'll use to predict new dataset\n",
    "### So let's define our predict function\n",
    "def predict(X, m):\n",
    "    y = np.dot(X, m.T)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.01600053,   1.34870013,   0.12619247,   0.57643535,\n",
       "        -2.28989842,   2.12620331,   0.12938467,  -3.17807284,\n",
       "         2.63684855,  -1.86959845,  -2.14517014,   0.67879373,\n",
       "        -3.93214294,  22.52227723])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's append a column of ones to our testing dataset as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_col_for_test = np.ones(len(X_test_scaled))\n",
    "ones_col_for_test = ones_col_for_test.reshape(-1, 1)\n",
    "ones_col_for_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 14)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = np.append(X_test_scaled, ones_col_for_test, axis=1) # let's append\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = predict(X_test_final, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32.64741185,  28.09520693,  18.02766328,  21.48478476,\n",
       "        18.8230408 ,  19.88427773,  32.41586396,  18.06649576,\n",
       "        24.42607583,  27.01399628,  27.04111375,  28.75176132,\n",
       "        21.16284384,  26.85720474,  23.39258691,  20.66444487,\n",
       "        17.32156481,  38.23622506,  30.51447773,   8.73915538,\n",
       "        20.80782236,  16.24240963,  25.22084507,  24.85310796,\n",
       "        31.39019724,  10.67193282,  13.78468589,  16.66704033,\n",
       "        36.52072924,  14.664133  ,  21.12635048,  13.95376748,\n",
       "        43.15215448,  17.96999701,  21.80958847,  20.58177959,\n",
       "        17.58080857,  27.2203981 ,   9.47660385,  19.82629589,\n",
       "        24.31559799,  21.18133992,  29.57334177,  16.32373913,\n",
       "        19.31195526,  14.51438568,  39.22236094,  18.11194946,\n",
       "        25.91089657,  20.29963459,  25.15143539,  24.42727121,\n",
       "        25.07538001,  26.6617263 ,   4.56534337,  24.08040157,\n",
       "        10.86627939,  26.89559998,  16.85446095,  35.87748353,\n",
       "        19.5471848 ,  27.52204504,  16.57872473,  18.73952126,\n",
       "        11.11965828,  32.36261644,  36.72080369,  21.97219568,\n",
       "        24.58710386,  25.16247393,  23.42962344,   6.92006898,\n",
       "        16.55432899,  20.41578021,  20.80455508,  21.54079212,\n",
       "        33.86017415,  27.95366121,  25.17387102,  34.65251862,\n",
       "        18.62566016,  23.9826695 ,  34.6463844 ,  13.30579098,\n",
       "        20.71859886,  30.08480283,  17.11473733,  24.3050481 ,\n",
       "        19.25744127,  16.98009879,  27.00806534,  41.84349519,\n",
       "        14.12718629,  23.26135385,  14.65273233,  21.87468237,\n",
       "        23.02374263,  29.09093203,  37.11070102,  20.53195293,\n",
       "        17.33354363,  17.7048918 ])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's create a score function to check the accuracy of our model\n",
    "def score(y_true, y_predicted):\n",
    "    u = ((y_true - y_predicted)**2).sum()\n",
    "    v = ((y_true - y_true.mean())**2).sum()\n",
    "    return (1 - u/v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76347292184108806"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = score(y_test, y_predicted)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ahaa not bad score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
